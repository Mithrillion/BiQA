{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improved Attentive Reader\n",
    "A thorough examination of the cnn/daily mail reading comprehension task\n",
    "\n",
    "Chen, Danqi, Jason Bolton, and Christopher D. Manning. \"A thorough examination of the cnn/daily mail reading comprehension task.\" arXiv preprint arXiv:1606.02858 (2016).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "R_i = \\phi_{RNN}^C(emb(C_i))[:]\n",
    "$$$$\n",
    "r = \\phi_{RNN}^Q(emb(Q))[-1]\n",
    "$$$$\n",
    "a_i = Softmax(R_i^T M r)\n",
    "$$$$\n",
    "u = \\sum_i a_i R_i\n",
    "$$$$\n",
    "o = Softmax(\\phi_{Linear}(u))\n",
    "$$\n",
    "\n",
    "Generate representations of story tokens and the question, calculate the similarity between tokens and the question, then map the attended word(s) back to the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Model Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shared Embedding Reader\n",
    "$$\n",
    "\\phi(\\cdot)=\\phi^{RNN}((\\phi^{word\\_emb_{L_1, L_2}}(\\cdot) + \\phi^{entity\\_emb}(\\cdot))\n",
    "$$$$\n",
    "R_i^{(j)}[:]=\\phi^{(C)}(C_i^{(j)})[:]\n",
    "$$$$\n",
    "r_i^{(j)}=\\phi^{(Q)}(Q_i^{(j)})[-1]\n",
    "$$$$\n",
    "\\gamma(R, r) = Softmax[Linear(r, \\sigma(R, r) \\odot R)]\n",
    "$$\n",
    "\n",
    "Shared embedding mechanism: WIP\n",
    "\n",
    "Goal:\n",
    "$$\n",
    "argmax_{\\Theta_\\phi, \\Theta_\\gamma} logP(A|Q, C, \\Theta_\\phi, \\Theta_\\gamma)\n",
    "$$\n",
    "\n",
    "Use embedding vectors mapped to a shared space for $L_1$ and $L_2$, use the same embeddings for entity tokens, then use the same network to train on both languages.\n",
    "\n",
    "#### Reader + Language Discriminator\n",
    "$$\n",
    "\\phi_i(\\cdot)=\\phi_i^{RNN}((\\phi_i^{word\\_emb}(\\cdot) + \\phi^{entity\\_emb}(\\cdot))\n",
    "$$$$\n",
    "R_i^{(j)}[:]=\\phi_i^{(C)}(C_i^{(j)})[:]\n",
    "$$$$\n",
    "r_i^{(j)}=\\phi_i^{(Q)}(Q_i^{(j)})[-1]\n",
    "$$$$\n",
    "\\gamma(R, r) = Softmax[Linear(r, \\sigma_i(R, r) \\odot R)]\n",
    "$$$$\n",
    "\\delta(\\cdot) = \\delta^{MLP}(\\cdot)\n",
    "$$\n",
    "\n",
    "The joint goal can be represented as:\n",
    "$$\n",
    "argmax_{\\Theta_\\phi, \\Theta_\\gamma} [logP(A|Q, C, \\Theta_\\phi, \\Theta_\\gamma)\n",
    "+ \\alpha \\cdot log P(\\lnot L|Q, C,\\Theta_\\phi, \\Theta_\\delta)]\n",
    "$$\n",
    "\n",
    "Use different (or shared) embeddings for each language, then before the answerer layer, also pass the output through a discriminator network which tries to identify the source language. Through adversarial training, force the representation before the answerer network to be language-independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Collection Method and Description\n",
    "\n",
    "#### English QA data:\n",
    "https://github.com/deepmind/rc-data/ (Hermann et al., NIPS 2015)\n",
    "\n",
    "Use CNN part of the dataset.\n",
    "Story - Question - Answer tuples, anonymised with coreference resolution.\n",
    "\n",
    "#### Spanish QA data:\n",
    "Collected from www.elmondo.es (via cached links on Wayback Machine) and processed to fit the format of the CNN dataset. Anonymised through named entity recognition.\n",
    "\n",
    "| **Dataset** | **English** | **Spanish** |\n",
    "|-----|-----|-----|\n",
    "| Number of articles | 0 | 0 |\n",
    "| SQA tuples | 0 | 0 |\n",
    "| Vocabulary size | 0 | 0 |\n",
    "| Total tokens | 0 | 0 |\n",
    "\n",
    "#### English and Spanish word embeddings:\n",
    "https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\n",
    "\n",
    "(P. Bojanowski*, E. Grave*, A. Joulin, T. Mikolov, Enriching Word Vectors with Subword Information)\n",
    "\n",
    "300 dimension word vectors trained on Wikipedia text.\n",
    "\n",
    "#### Word alignment dictionary:\n",
    "http://opus.lingfil.uu.se/download.php?f=EUbookshop%2Fdic%2Fen-es.dic\n",
    "\n",
    "(Raivis Skadiņš, J�rg Tiedemann, Roberts Rozis and Daiga Deksne (2014): Billions of Parallel Words for Free, In Proceedings of LREC 2014, Reykjavik, Iceland)\n",
    "\n",
    "EU Bookshop parallel corpus dataset\n",
    "\n",
    "http://opus.lingfil.uu.se/OpenSubtitles2012.php\n",
    "(Jörg Tiedemann, 2012, Parallel Data, Tools and Interfaces in OPUS. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012))\n",
    "\n",
    "Open Subtitles dataset (more in-corpus aligned words)\n",
    "\n",
    "| **Dictionary** | **EU Bookshop** | **Open Subtitles** |\n",
    "|-----|-----|-----|\n",
    "| Total aligned words | 0 | 0 |\n",
    "| In-corpus aligned words | 0 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the shared embedding reader, we perform the following experiments:\n",
    "\n",
    "1. Evaluating using original English and Spanish embeddings respectively.\n",
    "2. Evaluating using aligned embeddings for each language separately.\n",
    "3. Evaluating Spanish performance on model trained on English using mapped embeddings.\n",
    "4. Evaluating Spanish performance after training on Spanish data the model initialised by English training (both using original embeddings).\n",
    "5. Evaluating Spanish performance after training on Spanish data the model initialised by English training (both using mapped embeddings).\n",
    "6. Interleaving training of English and Spanish on the same model and evaluating on both languages (both using mapped embeddings).\n",
    "7. Using adversarial training with 6 without using mapped embeddings.\n",
    "8. Using adversarial training with 6 with using mapped embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method | English (best) | Spanish (best) | Bilingual (best) | English (avg.) | Spanish (avg.) | Bilingual (avg.) |\n",
    "|-----|-----|-----|-----|\n",
    "| Individual embeddings | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| Mapped embeddings | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| Adversarial training | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| Mapped embeddings / Adv. Training | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "\n",
    "\n",
    "| Mapping Method | English | Spanish | Bilingual |\n",
    "|-----|-----|-----|-----|\n",
    "| None | 0 | 0 | 0 |\n",
    "| Unconstrained | 0 | 0 | 0 |\n",
    "| Normalised + mean shift | 0 | 0 | 0 |\n",
    "\n",
    "#### Training Curve Comparison\n",
    "\n",
    "#### Dictionary Size vs Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
